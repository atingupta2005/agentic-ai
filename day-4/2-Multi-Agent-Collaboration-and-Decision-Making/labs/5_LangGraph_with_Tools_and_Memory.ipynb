{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: LangGraph with Tools, Conditional Logic, and Memory\n",
    "\n",
    "This lab builds on our first LangGraph application by introducing three powerful features that are essential for creating sophisticated agents:\n",
    "\n",
    "1.  **Tools**: We will create tools from Python functions and give our agent the ability to use them (e.g., to perform a web search or send a notification).\n",
    "2.  **Conditional Edges**: We will build a more complex graph that can dynamically route its logic. The agent will decide for itself whether to respond directly to the user or to first use a tool.\n",
    "3.  **Persistent Memory (Checkpoints)**: We will add a checkpointer to our graph, allowing it to save its state after each step. This gives our agent long-term memory, enabling it to recall previous turns in a conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup: External Tool APIs\n",
    "\n",
    "This lab uses two external services for its tools. You will need to get API keys for them and add them to your `.env` file.\n",
    "\n",
    "1.  **Serper for Google Search**: Go to [Serper.dev](https://serper.dev/) and sign up for a free account to get an API key.\n",
    "    ```\n",
    "    SERPER_API_KEY=\"YOUR_KEY_HERE\"\n",
    "    ```\n",
    "2.  **Pushover for Notifications (Optional)**: Go to [Pushover.net](https://pushover.net/) to get a User Key and create an Application to get an API Token.\n",
    "    ```\n",
    "    PUSHOVER_USER=\"YOUR_USER_KEY_HERE\"\n",
    "    PUSHOVER_TOKEN=\"YOUR_API_TOKEN_HERE\"\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports ===\n",
    "import os\n",
    "import requests\n",
    "import sqlite3\n",
    "from typing import Annotated, TypedDict\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain components for creating tools\n",
    "from langchain.agents import Tool\n",
    "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Core LangGraph components\n",
    "from langgraph.graph import StateGraph, START\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "# LangGraph components for adding persistent memory\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "# For the UI\n",
    "from IPython.display import Image, display\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Creating Tools for Our Agent\n",
    "\n",
    "A \"Tool\" is a function that an agent can decide to call. LangChain provides convenient wrappers to easily convert any Python function into a tool that an LLM can understand and use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Tool 1: Web Search ===\n",
    "# LangChain Community provides a simple wrapper for the Serper Google Search API.\n",
    "search_wrapper = GoogleSerperAPIWrapper()\n",
    "\n",
    "# We wrap the search function in a LangChain `Tool` object.\n",
    "# This adds a name and description, which the LLM uses to decide when to use the tool.\n",
    "search_tool = Tool(\n",
    "    name=\"search\",\n",
    "    func=search_wrapper.run,\n",
    "    description=\"Useful for when you need to answer questions about current events or look up information on the web.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Tool 2: Push Notification (Optional) ===\n",
    "# We can also create a tool from our own custom function.\n",
    "\n",
    "def send_push_notification(text: str):\n",
    "    \"\"\"Send a push notification to the user's device via Pushover.\"\"\"\n",
    "    pushover_token = os.getenv(\"PUSHOVER_TOKEN\")\n",
    "    pushover_user = os.getenv(\"PUSHOVER_USER\")\n",
    "    if pushover_token and pushover_user:\n",
    "        requests.post(\n",
    "            \"https://api.pushover.net/1/messages.json\", \n",
    "            data={\"token\": pushover_token, \"user\": pushover_user, \"message\": text}\n",
    "        )\n",
    "        return \"Notification sent successfully.\"\n",
    "    else:\n",
    "        return \"Pushover credentials not set. Could not send notification.\"\n",
    "\n",
    "push_tool = Tool(\n",
    "    name=\"send_push_notification\",\n",
    "    func=send_push_notification,\n",
    "    description=\"Use this tool to send a push notification to the user.\"\n",
    ")\n",
    "\n",
    "tools = [search_tool, push_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Building a Conditional Graph\n",
    "\n",
    "Now, we'll build a more advanced graph. Instead of a simple `START -> node -> END` flow, this graph will have a conditional edge. After the chatbot node runs, the graph will check if the LLM decided to call a tool. \n",
    "- If YES, it will route to a `ToolNode` to execute the tool.\n",
    "- If NO, it will route directly to the `END`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define the State\n",
    "# We'll use a TypedDict for the state this time, which is another common pattern.\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create the Graph Builder\n",
    "graph_builder = StateGraph(State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define the Nodes\n",
    "\n",
    "# First, we bind our tools to the LLM. This tells the LLM that these tools are available for it to call.\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "# The chatbot node remains simple: it just calls the LLM.\n",
    "def chatbot_node(state: State):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "# We also add a `ToolNode`. This is a pre-built node from LangGraph that knows how to execute tools.\n",
    "tool_node = ToolNode(tools=tools)\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot_node)\n",
    "graph_builder.add_node(\"tools\", tool_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Add Edges\n",
    "\n",
    "# The entry point is the chatbot.\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "\n",
    "# This is the conditional edge. After the 'chatbot' node, it calls `tools_condition`.\n",
    "# `tools_condition` is a built-in function that checks if the last message contains a tool call.\n",
    "# If it does, the graph transitions to the 'tools' node. Otherwise, it transitions to END.\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    tools_condition,\n",
    "    # The third argument is a dictionary mapping the condition's outcomes to the next node.\n",
    "    # In this case, if `tools_condition` returns \"tools\", we go to the \"tools\" node.\n",
    "    # If it returns anything else (like END), we end the graph.\n",
    "    {\n",
    "        \"tools\": \"tools\",\n",
    "        \"__end__\": \"__end__\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# After the tools are executed, we always loop back to the chatbot node to let it process the tool's output.\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Adding Persistent Memory with Checkpoints\n",
    "\n",
    "By default, a LangGraph's state is ephemeral and resets after each `.invoke()` call. To give our chatbot memory, we need to add a **checkpointer**. The checkpointer saves the state of the graph at every step, allowing us to resume conversations.\n",
    "\n",
    "We'll use `SqliteSaver` to store the conversation history in a local SQLite database file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The checkpointer connects to a SQLite database file.\n",
    "# `check_same_thread=False` is required for SQLite in this context.\n",
    "memory = SqliteSaver.from_conn_string(\"memory.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Compile the Graph with the Checkpointer\n",
    "# We pass the memory object to the `.compile()` method.\n",
    "graph = graph_builder.compile(checkpointer=memory)\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showtime! Running the Stateful Chatbot\n",
    "\n",
    "To use the memory, we need to pass a `config` object to the `.invoke()` call. The `thread_id` in the config tells LangGraph which conversation history to load and save. Each unique `thread_id` will have its own separate memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use a unique thread_id for this conversation.\n",
    "# You can change this ID to start a new, separate conversation.\n",
    "config = {\"configurable\": {\"thread_id\": \"my-first-thread\"}}\n",
    "\n",
    "def chat_interface_function(user_input: str, history: list):\n",
    "    # We invoke the graph with the user's message and the config object.\n",
    "    result = graph.invoke(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
    "        config=config\n",
    "    )\n",
    "    # The graph's state is automatically saved and loaded by the checkpointer.\n",
    "    return result[\"messages\"][-1].content\n",
    "\n",
    "gr.ChatInterface(\n",
    "    chat_interface_function, \n",
    "    title=\"LangGraph Agent with Tools and Memory\",\n",
    "    description=\"Ask me a question that requires a web search, or ask me to send a notification!\",\n",
    "    examples=[\"What is the latest news on AI?\", \"Send me a notification that says 'Hello from my agent!'\"]\n",
    ").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can inspect the state history of any conversation thread at any time.\n",
    "print(\"--- Conversation History for thread 'my-first-thread' ---\")\n",
    "for state in graph.get_state_history(config):\n",
    "    print(state.values['messages'][-1])\n",
    "    print(\"---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
